{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-DeWfZdCl6-b"
      },
      "source": [
        "# CommentSense: AI-Powered Comment Analysis System\n",
        "\n",
        "**Problem Statement**: Measuring content effectiveness through Share of Engagement (SoE) metrics like likes, shares, saves, and comments is essential. How do we analyze the quality and relevance of comments, at scale?\n",
        "\n",
        "**Solution Features**:\n",
        "- Quality comment ratio analysis\n",
        "- Sentiment breakdown per video\n",
        "- Comment categorization (skincare, fragrance, makeup)\n",
        "- Spam detection\n",
        "- Relevance analysis using distance metrics\n",
        "\n",
        "By: **Noog Troupers**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KaiXPETwl6-f"
      },
      "source": [
        "## 1. Import Libraries and Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-09-11T10:14:38.109924Z",
          "start_time": "2025-09-11T10:14:14.581905Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HdNWzLGDl6-g",
        "outputId": "30d67337-cccf-4cb8-9716-635f7d75345a"
      },
      "source": [
        "%load_ext cudf.pandas\n",
        "!pip install aiopandas\n",
        "import aiopandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "import torch\n",
        "from transformers import pipeline, AutoTokenizer, AutoModel\n",
        "import nltk\n",
        "import re\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.cluster import KMeans\n",
        "from collections import Counter\n",
        "!pip install googletrans # for Google Colab\n",
        "from googletrans import Translator\n",
        "import asyncio\n",
        "!pip install langdetect\n",
        "from langdetect import detect\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style for better visualizations\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# Download NLTK data\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('stopwords', quiet=True)\n",
        "nltk.download('punkt_tab', quiet=True)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The cudf.pandas extension is already loaded. To reload it, use:\n",
            "  %reload_ext cudf.pandas\n",
            "Requirement already satisfied: aiopandas in /usr/local/lib/python3.12/dist-packages (0.0.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from aiopandas) (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas->aiopandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->aiopandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->aiopandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->aiopandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->aiopandas) (1.17.0)\n",
            "Requirement already satisfied: googletrans in /usr/local/lib/python3.12/dist-packages (4.0.2)\n",
            "Requirement already satisfied: httpx>=0.27.2 in /usr/local/lib/python3.12/dist-packages (from httpx[http2]>=0.27.2->googletrans) (0.28.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.2->httpx[http2]>=0.27.2->googletrans) (4.10.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.2->httpx[http2]>=0.27.2->googletrans) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.2->httpx[http2]>=0.27.2->googletrans) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.2->httpx[http2]>=0.27.2->googletrans) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.27.2->httpx[http2]>=0.27.2->googletrans) (0.16.0)\n",
            "Requirement already satisfied: h2<5,>=3 in /usr/local/lib/python3.12/dist-packages (from httpx[http2]>=0.27.2->googletrans) (4.3.0)\n",
            "Requirement already satisfied: hyperframe<7,>=6.1 in /usr/local/lib/python3.12/dist-packages (from h2<5,>=3->httpx[http2]>=0.27.2->googletrans) (6.1.0)\n",
            "Requirement already satisfied: hpack<5,>=4.1 in /usr/local/lib/python3.12/dist-packages (from h2<5,>=3->httpx[http2]>=0.27.2->googletrans) (4.1.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.27.2->httpx[http2]>=0.27.2->googletrans) (1.3.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.27.2->httpx[http2]>=0.27.2->googletrans) (4.15.0)\n",
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.12/dist-packages (1.0.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from langdetect) (1.17.0)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ],
      "execution_count": 47
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7wv0fVOl6-i"
      },
      "source": [
        "## 2. Data Loading Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-09-11T10:14:38.270245Z",
          "start_time": "2025-09-11T10:14:38.256242Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9PX0ooOl6-i",
        "outputId": "3e19a37b-9b19-4faf-81ce-b72a53a77659"
      },
      "source": [
        "class Dataset:\n",
        "    comment_links = [\n",
        "        \"https://storage.googleapis.com/dataset_hosting/comments1.csv\",\n",
        "        \"https://storage.googleapis.com/dataset_hosting/comments2.csv\",\n",
        "        \"https://storage.googleapis.com/dataset_hosting/comments3.csv\",\n",
        "        \"https://storage.googleapis.com/dataset_hosting/comments4.csv\",\n",
        "        \"https://storage.googleapis.com/dataset_hosting/comments5.csv\",\n",
        "    ]\n",
        "\n",
        "    video_link = \"https://storage.googleapis.com/dataset_hosting/videos.csv\"\n",
        "\n",
        "    @staticmethod\n",
        "    def getAllComments():\n",
        "        list_of_dfs = []\n",
        "        for csv_file in Dataset.comment_links:\n",
        "            df = pd.read_csv(csv_file)\n",
        "            list_of_dfs.append(df)\n",
        "        return pd.concat(list_of_dfs, ignore_index=True)\n",
        "\n",
        "    @staticmethod\n",
        "    def getComments(dataset_id=1, sample_frac=0.1):\n",
        "        if dataset_id not in range(1, len(Dataset.comment_links) + 1):\n",
        "            raise ValueError(f\"dataset_id must be between 1 and {len(Dataset.comment_links)}\")\n",
        "\n",
        "        df = pd.read_csv(Dataset.comment_links[dataset_id - 1])\n",
        "        if sample_frac < 1.0:\n",
        "            df = df.sample(frac=sample_frac, random_state=42)\n",
        "        return df\n",
        "\n",
        "    @staticmethod\n",
        "    def getVideos():\n",
        "        return pd.read_csv(Dataset.video_link)\n",
        "\n",
        "# Initialize dataset\n",
        "dataset = Dataset()\n",
        "print(\"Dataset class initialized successfully!\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset class initialized successfully!\n"
          ]
        }
      ],
      "execution_count": 48
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HkvZupRkl6-j"
      },
      "source": [
        "## 3. Advanced Text Preprocessing and Analysis Classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-09-11T10:14:38.298761Z",
          "start_time": "2025-09-11T10:14:38.283761Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yDZdOrJMl6-k",
        "outputId": "3fc621a1-88f4-4ac6-961d-e5039729b49c"
      },
      "source": [
        "class AdvancedTextPreprocessor:\n",
        "    def __init__(self):\n",
        "        self.stemmer = PorterStemmer()\n",
        "        self.stop_words = set(stopwords.words('english'))\n",
        "\n",
        "        # Beauty category keywords\n",
        "        self.category_keywords = {\n",
        "            'skincare': ['skincare', 'skin', 'moisturizer', 'cleanser', 'serum', 'cream', 'lotion',\n",
        "                        'acne', 'pores', 'wrinkles', 'anti-aging', 'hydrating', 'dry skin', 'oily skin',\n",
        "                        'sensitive skin', 'sunscreen', 'spf', 'retinol', 'vitamin c', 'hyaluronic',\n",
        "                        'exfoliate', 'toner', 'mask', 'facial', 'dermatologist'],\n",
        "\n",
        "            'makeup': ['makeup', 'foundation', 'concealer', 'lipstick', 'eyeshadow', 'mascara',\n",
        "                      'eyeliner', 'blush', 'bronzer', 'highlighter', 'primer', 'setting spray',\n",
        "                      'powder', 'contour', 'brow', 'eyebrow', 'lip gloss', 'lip liner', 'palette',\n",
        "                      'brush', 'beauty blender', 'sponge', 'coverage', 'matte', 'dewy', 'shimmer'],\n",
        "\n",
        "            'fragrance': ['perfume', 'fragrance', 'cologne', 'scent', 'smell', 'aroma', 'notes',\n",
        "                         'floral', 'woody', 'citrus', 'vanilla', 'musk', 'fresh', 'sweet', 'spicy',\n",
        "                         'eau de toilette', 'eau de parfum', 'body spray', 'long lasting',\n",
        "                         'signature scent', 'top notes', 'base notes', 'middle notes']\n",
        "        }\n",
        "\n",
        "        # Spam indicators\n",
        "        self.spam_keywords = [\n",
        "            'buy now', 'click here', 'subscribe', 'free', 'visit', 'winner', 'win', 'cash', 'prize',\n",
        "            'limited time', 'act now', 'urgent', 'amazing deal', 'check out my', 'follow me',\n",
        "            'dm me', 'link in bio', 'promo code', 'discount', '50% off', 'sale'\n",
        "        ]\n",
        "\n",
        "    def clean_text(self, text):\n",
        "        \"\"\"Clean and preprocess text\"\"\"\n",
        "        if pd.isna(text) or not isinstance(text, str):\n",
        "            return \"\"\n",
        "\n",
        "        # Convert to lowercase and strip\n",
        "        text = str(text).lower().strip()\n",
        "\n",
        "        # Remove URLs, mentions, hashtags\n",
        "        text = re.sub(r'http\\S+|www\\S+|@\\w+|#\\w+', '', text)\n",
        "\n",
        "        # Remove extra whitespace\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "        # Tokenize\n",
        "        tokens = word_tokenize(text)\n",
        "\n",
        "        # Remove stopwords and punctuation\n",
        "        tokens = [word for word in tokens if word not in self.stop_words and word not in string.punctuation]\n",
        "\n",
        "        # Stem tokens\n",
        "        tokens = [self.stemmer.stem(word) for word in tokens if len(word) > 2]\n",
        "\n",
        "        return ' '.join(tokens)\n",
        "\n",
        "    async def translate_text(self, text):\n",
        "        \"\"\"Translate text to English\"\"\"\n",
        "        # Detect if text is English, if so no translation needed\n",
        "        try:\n",
        "            if detect(text) == 'en':\n",
        "                return text\n",
        "        except Exception as e:\n",
        "            return text\n",
        "\n",
        "        # If the text is not English translate it to English\n",
        "        async with Translator() as translator:\n",
        "          try:\n",
        "              translated_obj = await translator.translate(text, dest='en')\n",
        "              return translated_obj.text\n",
        "          except Exception as e:\n",
        "              print(f\"Translation error: {e}\")\n",
        "              return text\n",
        "\n",
        "    def detect_spam(self, text):\n",
        "        \"\"\"Detect spam comments with improved logic\"\"\"\n",
        "        if pd.isna(text) or not isinstance(text, str):\n",
        "            return 1\n",
        "\n",
        "        text = str(text).lower()\n",
        "\n",
        "        # Remove emojis for length check\n",
        "        emoji_pattern = re.compile(\"[\"\n",
        "            u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "            u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "            u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "            u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "            \"]+\", flags=re.UNICODE)\n",
        "\n",
        "        text_no_emoji = emoji_pattern.sub('', text)\n",
        "\n",
        "        # Check for very short comments (likely spam/low quality)\n",
        "        if len(text_no_emoji.strip()) < 3:\n",
        "            return 1\n",
        "\n",
        "        # Check for excessive repetition\n",
        "        words = text_no_emoji.split()\n",
        "        if len(words) > 1 and len(set(words)) / len(words) < 0.5:\n",
        "            return 1\n",
        "\n",
        "        # Check for spam keywords\n",
        "        spam_score = sum(1 for keyword in self.spam_keywords if keyword in text)\n",
        "        if spam_score >= 2:\n",
        "            return 1\n",
        "\n",
        "        # Check for excessive caps\n",
        "        if len(text) > 10 and sum(1 for c in text if c.isupper()) / len(text) > 0.7:\n",
        "            return 1\n",
        "\n",
        "        return 0\n",
        "\n",
        "    def categorize_comment(self, text):\n",
        "        \"\"\"Categorize comments into beauty categories\"\"\"\n",
        "        if pd.isna(text) or not isinstance(text, str):\n",
        "            return 'other'\n",
        "\n",
        "        text = str(text).lower()\n",
        "        category_scores = {}\n",
        "\n",
        "        for category, keywords in self.category_keywords.items():\n",
        "            score = sum(1 for keyword in keywords if keyword in text)\n",
        "            category_scores[category] = score\n",
        "\n",
        "        if max(category_scores.values()) == 0:\n",
        "            return 'other'\n",
        "\n",
        "        return max(category_scores.keys(), key=category_scores.get)\n",
        "\n",
        "    def assess_quality(self, text, sentiment=None):\n",
        "        \"\"\"Assess comment quality based on multiple factors\"\"\"\n",
        "        if pd.isna(text) or not isinstance(text, str):\n",
        "            return 0\n",
        "\n",
        "        text = str(text).lower()\n",
        "        quality_score = 0\n",
        "\n",
        "        # Length factor (reasonable length comments are better)\n",
        "        word_count = len(text.split())\n",
        "        if 5 <= word_count <= 50:\n",
        "            quality_score += 2\n",
        "        elif 3 <= word_count < 5 or 50 < word_count <= 100:\n",
        "            quality_score += 1\n",
        "\n",
        "        # Product relevance\n",
        "        for keywords in self.category_keywords.values():\n",
        "            if any(keyword in text for keyword in keywords):\n",
        "                quality_score += 2\n",
        "                break\n",
        "\n",
        "        # Sentiment consideration\n",
        "        if sentiment and sentiment != 'neutral':\n",
        "            quality_score += 1\n",
        "\n",
        "        # Engagement indicators\n",
        "        engagement_words = ['love', 'amazing', 'recommend', 'favorite', 'best', 'great', 'good', 'bad', 'disappointed']\n",
        "        if any(word in text for word in engagement_words):\n",
        "            quality_score += 1\n",
        "\n",
        "        # Quality threshold\n",
        "        return 1 if quality_score >= 3 else 0\n",
        "\n",
        "print(\"AdvancedTextPreprocessor class created successfully!\")\n",
        "\n",
        "atp = AdvancedTextPreprocessor()\n",
        "print(await atp.translate_text(\"I don't like this product.\"))\n",
        "print(await atp.translate_text(\"su'ega su'ega su'ega\"))\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AdvancedTextPreprocessor class created successfully!\n",
            "I don't like this product.\n",
            "The picture is a picture is a picture is a picture is a picture is a picture\n"
          ]
        }
      ],
      "execution_count": 49
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9pXp-G_5l6-l"
      },
      "source": [
        "## 4. Relevance Analysis Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-09-11T10:14:38.348552Z",
          "start_time": "2025-09-11T10:14:38.339722Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FDVi-c-3l6-l",
        "outputId": "cd3ac53c-be4d-440a-dc8c-9d2769d0b0a4"
      },
      "source": [
        "class RelevanceAnalyzer:\n",
        "    def __init__(self):\n",
        "        self.vectorizer = TfidfVectorizer(max_features=1000, stop_words='english', ngram_range=(1, 2))\n",
        "\n",
        "    def calculate_relevance_score(self, comment_text, video_title, video_description=\"\", video_tags=\"\"):\n",
        "        \"\"\"Calculate relevance score using cosine similarity\"\"\"\n",
        "        try:\n",
        "            # Combine video content\n",
        "            video_content = f\"{video_title} {video_description} {video_tags}\".strip()\n",
        "\n",
        "            if not comment_text or not video_content:\n",
        "                return 0.0\n",
        "\n",
        "            # Create TF-IDF vectors\n",
        "            texts = [str(comment_text), str(video_content)]\n",
        "            tfidf_matrix = self.vectorizer.fit_transform(texts)\n",
        "\n",
        "            # Calculate cosine similarity\n",
        "            similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]\n",
        "\n",
        "            return float(similarity)\n",
        "        except:\n",
        "            return 0.0\n",
        "\n",
        "    def batch_relevance_analysis(self, comments_df, videos_df):\n",
        "        \"\"\"Perform batch relevance analysis\"\"\"\n",
        "        # Merge comments with video data\n",
        "        merged_df = comments_df.merge(videos_df[['videoId', 'title', 'description', 'tags']],\n",
        "                                     on='videoId', how='left')\n",
        "\n",
        "        # Calculate relevance scores\n",
        "        relevance_scores = []\n",
        "        for _, row in merged_df.iterrows():\n",
        "            score = self.calculate_relevance_score(\n",
        "                row.get('textOriginal', ''),\n",
        "                row.get('title', ''),\n",
        "                row.get('description', ''),\n",
        "                row.get('tags', '')\n",
        "            )\n",
        "            relevance_scores.append(score)\n",
        "\n",
        "        return relevance_scores\n",
        "\n",
        "print(\"RelevanceAnalyzer class created successfully!\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RelevanceAnalyzer class created successfully!\n"
          ]
        }
      ],
      "execution_count": 50
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjDznNazl6-m"
      },
      "source": [
        "## 5. Visualization Dashboard Class"
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-09-11T11:50:13.905726Z",
          "start_time": "2025-09-11T11:50:13.890235Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ajNlTKbel6-m",
        "outputId": "c8054bea-f2c7-432f-84b7-01210b8f78ae"
      },
      "cell_type": "code",
      "source": [
        "class CommentAnalyticsDashboard:\n",
        "    def __init__(self):\n",
        "        self.colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FFEAA7', '#DDA0DD']\n",
        "\n",
        "    def create_quality_ratio_chart(self, df):\n",
        "        \"\"\"Create quality ratio visualization\"\"\"\n",
        "        quality_counts = df['quality_score'].value_counts()\n",
        "\n",
        "        fig = go.Figure(data=[\n",
        "            go.Pie(labels=['Low Quality', 'High Quality'],\n",
        "                   values=[quality_counts.get(0, 0), quality_counts.get(1, 0)],\n",
        "                   hole=0.4,\n",
        "                   marker_colors=['#FF6B6B', '#4ECDC4'])\n",
        "        ])\n",
        "\n",
        "        fig.update_layout(\n",
        "            title=\"Comment Quality Ratio\",\n",
        "            annotations=[dict(text='Quality<br>Ratio', x=0.5, y=0.5, font_size=20, showarrow=False)]\n",
        "        )\n",
        "\n",
        "        return fig\n",
        "\n",
        "    def create_sentiment_breakdown(self, df):\n",
        "        \"\"\"Create sentiment breakdown visualization\"\"\"\n",
        "        sentiment_counts = df['sentiment'].value_counts()\n",
        "\n",
        "        fig = px.bar(x=sentiment_counts.index, y=sentiment_counts.values,\n",
        "                     title=\"Sentiment Distribution\",\n",
        "                     labels={'x': 'Sentiment', 'y': 'Count'},\n",
        "                     color=sentiment_counts.index,\n",
        "                     color_discrete_sequence=self.colors)\n",
        "\n",
        "        fig.update_layout(showlegend=False)\n",
        "        return fig\n",
        "\n",
        "    def create_category_breakdown(self, df):\n",
        "        \"\"\"Create category breakdown visualization\"\"\"\n",
        "        category_counts = df['category'].value_counts()\n",
        "\n",
        "        fig = px.pie(values=category_counts.values, names=category_counts.index,\n",
        "                     title=\"Comment Categories\",\n",
        "                     color_discrete_sequence=self.colors)\n",
        "\n",
        "        return fig\n",
        "\n",
        "    def create_spam_detection_chart(self, df):\n",
        "        \"\"\"Create spam detection visualization\"\"\"\n",
        "        spam_counts = df['isSpam'].value_counts()\n",
        "\n",
        "        fig = go.Figure(data=[\n",
        "            go.Bar(x=['Legitimate', 'Spam'],\n",
        "                   y=[spam_counts.get(0, 0), spam_counts.get(1, 0)],\n",
        "                   marker_color=['#4ECDC4', '#FF6B6B'])\n",
        "        ])\n",
        "\n",
        "        fig.update_layout(title=\"Spam Detection Results\")\n",
        "        return fig\n",
        "\n",
        "    def create_relevance_distribution(self, df):\n",
        "        \"\"\"Create relevance score distribution\"\"\"\n",
        "        fig = px.histogram(df, x='relevance_score',\n",
        "                          title=\"Comment Relevance Score Distribution\",\n",
        "                          labels={'x': 'Relevance Score', 'y': 'Count'})\n",
        "\n",
        "        fig.add_vline(x=df['relevance_score'].mean(), line_dash=\"dash\",\n",
        "                     annotation_text=f\"Mean: {df['relevance_score'].mean():.3f}\")\n",
        "\n",
        "        return fig\n",
        "\n",
        "    def create_video_analysis_summary(self, df, video_id):\n",
        "        \"\"\"Create per-video analysis summary\"\"\"\n",
        "        video_data = df[df['videoId'] == video_id]\n",
        "\n",
        "        if len(video_data) == 0:\n",
        "            return None\n",
        "\n",
        "        # Create subplot figure\n",
        "        fig = make_subplots(\n",
        "            rows=2, cols=2,\n",
        "            subplot_titles=('Quality Ratio', 'Sentiment Distribution',\n",
        "                           'Category Breakdown', 'Relevance Scores'),\n",
        "            specs=[[{'type': 'domain'}, {'type': 'xy'}],\n",
        "                   [{'type': 'domain'}, {'type': 'xy'}]]\n",
        "        )\n",
        "\n",
        "        # Quality ratio pie chart\n",
        "        quality_counts = video_data['quality_score'].value_counts()\n",
        "        fig.add_trace(go.Pie(labels=['Low Quality', 'High Quality'],\n",
        "                            values=[quality_counts.get(0, 0), quality_counts.get(1, 0)],\n",
        "                            name=\"Quality\"), row=1, col=1)\n",
        "\n",
        "        # Sentiment bar chart\n",
        "        sentiment_counts = video_data['sentiment'].value_counts()\n",
        "        fig.add_trace(go.Bar(x=sentiment_counts.index, y=sentiment_counts.values,\n",
        "                            name=\"Sentiment\"), row=1, col=2)\n",
        "\n",
        "        # Category pie chart\n",
        "        category_counts = video_data['category'].value_counts()\n",
        "        fig.add_trace(go.Pie(labels=category_counts.index, values=category_counts.values,\n",
        "                            name=\"Category\"), row=2, col=1)\n",
        "\n",
        "        # Relevance histogram\n",
        "        fig.add_trace(go.Histogram(x=video_data['relevance_score'], name=\"Relevance\"),\n",
        "                     row=2, col=2)\n",
        "\n",
        "        fig.update_layout(height=800, title_text=f\"Video Analysis Summary - {video_id}\")\n",
        "\n",
        "        return fig\n",
        "\n",
        "print(\"CommentAnalyticsDashboard class created successfully!\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CommentAnalyticsDashboard class created successfully!\n"
          ]
        }
      ],
      "execution_count": 51
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ca3kts-Jl6-n"
      },
      "source": [
        "## 6. Load and Preprocess Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-09-11T10:15:40.120043Z",
          "start_time": "2025-09-11T10:14:38.438462Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WHAFzJIql6-n",
        "outputId": "dbdcea46-aea0-4f6e-e9ef-20482761b85a"
      },
      "source": [
        "# Load datasets\n",
        "print(\"Loading video dataset...\")\n",
        "videos = dataset.getVideos()\n",
        "\n",
        "print(\"Loading comments dataset (10% sample for demo)...\")\n",
        "comments = dataset.getComments(dataset_id=1, sample_frac=0.1)\n",
        "\n",
        "print(f\"Loaded {len(videos)} videos and {len(comments)} comments\")\n",
        "\n",
        "# Remove duplicates\n",
        "comments = comments.drop_duplicates(subset=[\"commentId\"])\n",
        "videos = videos.drop_duplicates(subset=[\"videoId\"])\n",
        "\n",
        "# Drop rows with missing comment text\n",
        "comments = comments.dropna(subset=[\"textOriginal\"])\n",
        "\n",
        "print(f\"After cleaning: {len(videos)} videos and {len(comments)} comments\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading video dataset...\n",
            "Loading comments dataset (10% sample for demo)...\n",
            "Loaded 92759 videos and 100000 comments\n",
            "After cleaning: 92759 videos and 99997 comments\n"
          ]
        }
      ],
      "execution_count": 52
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BQn6O-gl6-n"
      },
      "source": [
        "## 7. Advanced Text Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-09-11T10:15:58.588843Z",
          "start_time": "2025-09-11T10:15:40.197404Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p6px3n1ol6-o",
        "outputId": "dc20e277-0d94-446d-ec24-b498b35320c1"
      },
      "source": [
        "# Initialize text preprocessor\n",
        "preprocessor = AdvancedTextPreprocessor()\n",
        "\n",
        "print(\"Performing text preprocessing and analysis...\")\n",
        "\n",
        "# Detect language and translate text to English\n",
        "print(\"Detecting and translating text to English...\")\n",
        "comments[\"textTranslated\"] = await comments[\"textOriginal\"].aapply(preprocessor.translate_text)\n",
        "\n",
        "# Clean text\n",
        "print(\"Cleaning text...\")\n",
        "comments[\"textCleaned\"] = comments[\"textOriginal\"].apply(preprocessor.clean_text)\n",
        "\n",
        "# Detect spam\n",
        "print(\"Detecting spam comments...\")\n",
        "comments[\"isSpam\"] = comments[\"textOriginal\"].apply(preprocessor.detect_spam)\n",
        "\n",
        "# Categorize comments\n",
        "print(\"Categorizing comments...\")\n",
        "comments[\"category\"] = comments[\"textOriginal\"].apply(preprocessor.categorize_comment)\n",
        "\n",
        "print(\"Text preprocessing completed!\")\n",
        "print(f\"Spam comments detected: {comments['isSpam'].sum()} ({comments['isSpam'].mean()*100:.1f}%)\")\n",
        "print(\"\\nCategory distribution:\")\n",
        "print(comments['category'].value_counts())"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing text preprocessing and analysis...\n",
            "Detecting and translating text to English...\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRM77MXVl6-o"
      },
      "source": [
        "## 8. Sentiment Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-09-11T10:32:51.109073Z",
          "start_time": "2025-09-11T10:15:58.600867Z"
        },
        "id": "K3VcufFcl6-o"
      },
      "source": [
        "# Setup sentiment analysis pipeline\n",
        "device_index = 0 if torch.cuda.is_available() else -1\n",
        "model_name = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
        "\n",
        "print(f\"Initializing sentiment analysis model: {model_name}\")\n",
        "print(f\"Using device: {'GPU' if device_index == 0 else 'CPU'}\")\n",
        "\n",
        "analyzer = pipeline(\n",
        "    \"sentiment-analysis\",\n",
        "    model=model_name,\n",
        "    truncation=True,\n",
        "    device=device_index,\n",
        ")\n",
        "\n",
        "# Perform batch sentiment analysis\n",
        "print(\"Performing sentiment analysis...\")\n",
        "\n",
        "# Prepare texts for analysis\n",
        "texts_series = comments[\"textCleaned\"].fillna(\"\").astype(str)\n",
        "unique_texts = list(pd.Series(texts_series.unique()))\n",
        "\n",
        "# Batch processing to avoid memory issues\n",
        "batch_size = 64\n",
        "label_map = {}\n",
        "score_map = {}\n",
        "\n",
        "for i in range(0, len(unique_texts), batch_size):\n",
        "    batch = unique_texts[i:i + batch_size]\n",
        "    try:\n",
        "        results = analyzer(batch, truncation=True, max_length=512)\n",
        "        for text, result in zip(batch, results):\n",
        "            label_map[text] = result[\"label\"]\n",
        "            score_map[text] = result[\"score\"]\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing batch {i//batch_size + 1}: {e}\")\n",
        "        # Handle failed batch by assigning neutral sentiment\n",
        "        for text in batch:\n",
        "            label_map[text] = \"neutral\"\n",
        "            score_map[text] = 0.5\n",
        "\n",
        "# Map results back to dataframe\n",
        "comments[\"sentiment\"] = texts_series.map(label_map)\n",
        "comments[\"sentiment_score\"] = texts_series.map(score_map)\n",
        "\n",
        "print(\"Sentiment analysis completed!\")\n",
        "print(\"\\nSentiment distribution:\")\n",
        "print(comments['sentiment'].value_counts())"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1q29hCtl6-p"
      },
      "source": [
        "## 9. Quality Assessment and Relevance Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-09-11T10:35:42.214820Z",
          "start_time": "2025-09-11T10:32:51.363546Z"
        },
        "id": "pSv7yQOUl6-p"
      },
      "source": [
        "# Quality assessment\n",
        "print(\"Assessing comment quality...\")\n",
        "comments[\"quality_score\"] = comments.apply(\n",
        "    lambda row: preprocessor.assess_quality(row[\"textOriginal\"], row[\"sentiment\"]), axis=1\n",
        ")\n",
        "\n",
        "# Relevance analysis\n",
        "print(\"Performing relevance analysis...\")\n",
        "relevance_analyzer = RelevanceAnalyzer()\n",
        "comments[\"relevance_score\"] = relevance_analyzer.batch_relevance_analysis(comments, videos)\n",
        "\n",
        "print(\"Quality assessment and relevance analysis completed!\")\n",
        "print(f\"High quality comments: {comments['quality_score'].sum()} ({comments['quality_score'].mean()*100:.1f}%)\")\n",
        "print(f\"Average relevance score: {comments['relevance_score'].mean():.3f}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NoQ3mrsdl6-p"
      },
      "source": [
        "## 10. Key Performance Indicators (KPIs)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-09-11T10:35:42.384078Z",
          "start_time": "2025-09-11T10:35:42.336632Z"
        },
        "id": "SnTJmatvl6-q"
      },
      "source": [
        "def calculate_kpis(df):\n",
        "    \"\"\"Calculate key performance indicators\"\"\"\n",
        "    total_comments = len(df)\n",
        "\n",
        "    kpis = {\n",
        "        'Total Comments': total_comments,\n",
        "        'Quality Comment Ratio': df['quality_score'].mean(),\n",
        "        'Spam Rate': df['isSpam'].mean(),\n",
        "        'Average Relevance Score': df['relevance_score'].mean(),\n",
        "        'Positive Sentiment %': (df['sentiment'] == 'positive').mean() * 100,\n",
        "        'Negative Sentiment %': (df['sentiment'] == 'negative').mean() * 100,\n",
        "        'Neutral Sentiment %': (df['sentiment'] == 'neutral').mean() * 100,\n",
        "        'Skincare Comments %': (df['category'] == 'skincare').mean() * 100,\n",
        "        'Makeup Comments %': (df['category'] == 'makeup').mean() * 100,\n",
        "        'Fragrance Comments %': (df['category'] == 'fragrance').mean() * 100,\n",
        "        'Other Comments %': (df['category'] == 'other').mean() * 100\n",
        "    }\n",
        "\n",
        "    return kpis\n",
        "\n",
        "# Calculate overall KPIs\n",
        "overall_kpis = calculate_kpis(comments)\n",
        "\n",
        "print(\"=== COMMENT ANALYSIS KPIs ===\")\n",
        "for kpi, value in overall_kpis.items():\n",
        "    if '%' in kpi or 'Ratio' in kpi or 'Rate' in kpi or 'Score' in kpi:\n",
        "        print(f\"{kpi}: {value:.2f}%\" if '%' in kpi else f\"{kpi}: {value:.3f}\")\n",
        "    else:\n",
        "        print(f\"{kpi}: {value:,}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkTckwgAl6-r"
      },
      "source": [
        "## 11. Create Interactive Dashboard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-09-11T11:50:20.626270Z",
          "start_time": "2025-09-11T11:50:20.305621Z"
        },
        "id": "E7P_W-kjl6-r"
      },
      "source": [
        "# Initialize dashboard\n",
        "dashboard = CommentAnalyticsDashboard()\n",
        "\n",
        "# Create visualizations\n",
        "print(\"Creating interactive dashboard...\")\n",
        "\n",
        "# Overall quality ratio\n",
        "quality_fig = dashboard.create_quality_ratio_chart(comments)\n",
        "quality_fig.show()\n",
        "\n",
        "# Sentiment breakdown\n",
        "sentiment_fig = dashboard.create_sentiment_breakdown(comments)\n",
        "sentiment_fig.show()\n",
        "\n",
        "# Category breakdown\n",
        "category_fig = dashboard.create_category_breakdown(comments)\n",
        "category_fig.show()\n",
        "\n",
        "# Spam detection\n",
        "spam_fig = dashboard.create_spam_detection_chart(comments)\n",
        "spam_fig.show()\n",
        "\n",
        "# Relevance distribution\n",
        "relevance_fig = dashboard.create_relevance_distribution(comments)\n",
        "relevance_fig.show()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJk8tnIhl6-r"
      },
      "source": [
        "## 12. Per-Video Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-09-11T11:50:29.805142Z",
          "start_time": "2025-09-11T11:50:29.739261Z"
        },
        "id": "IB3qjNZZl6-s"
      },
      "source": [
        "# Get video-level analytics\n",
        "def get_video_analytics(df):\n",
        "    \"\"\"Generate per-video analytics\"\"\"\n",
        "    video_stats = df.groupby('videoId').agg({\n",
        "        'commentId': 'count',\n",
        "        'quality_score': 'mean',\n",
        "        'isSpam': 'mean',\n",
        "        'relevance_score': 'mean',\n",
        "        'sentiment_score': 'mean'\n",
        "    }).round(3)\n",
        "\n",
        "    video_stats.columns = ['Total_Comments', 'Quality_Ratio', 'Spam_Rate', 'Avg_Relevance', 'Avg_Sentiment_Score']\n",
        "    video_stats = video_stats.sort_values('Total_Comments', ascending=False)\n",
        "\n",
        "    return video_stats\n",
        "\n",
        "video_analytics = get_video_analytics(comments)\n",
        "print(\"=== TOP 10 VIDEOS BY COMMENT COUNT ===\")\n",
        "print(video_analytics.head(10))\n",
        "\n",
        "# Analyze a specific video\n",
        "top_video_id = video_analytics.index[0]\n",
        "print(f\"\\n=== DETAILED ANALYSIS FOR TOP VIDEO: {top_video_id} ===\")\n",
        "\n",
        "video_summary_fig = dashboard.create_video_analysis_summary(comments, top_video_id)\n",
        "if video_summary_fig:\n",
        "    video_summary_fig.show()\n",
        "\n",
        "# Show sample high-quality comments for the top video\n",
        "top_video_comments = comments[comments['videoId'] == top_video_id]\n",
        "high_quality_comments = top_video_comments[\n",
        "    (top_video_comments['quality_score'] == 1) &\n",
        "    (top_video_comments['isSpam'] == 0)\n",
        "].sort_values('relevance_score', ascending=False)\n",
        "\n",
        "print(f\"\\n=== SAMPLE HIGH-QUALITY COMMENTS FROM {top_video_id} ===\")\n",
        "for i, (_, comment) in enumerate(high_quality_comments.head(5).iterrows()):\n",
        "    print(f\"{i+1}. [{comment['sentiment'].upper()}] (Relevance: {comment['relevance_score']:.3f})\")\n",
        "    print(f\"   \\\"{comment['textOriginal'][:100]}...\\\"\")\n",
        "    print(f\"   Category: {comment['category']}\\n\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqi4JU_Sl6-s"
      },
      "source": [
        "## 13. Advanced Analytics and Insights"
      ]
    },
    {
      "metadata": {
        "id": "T0OyiMkll6-s"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "# Create comprehensive insights\n",
        "def generate_insights(df):\n",
        "    \"\"\"Generate actionable insights from the data\"\"\"\n",
        "    insights = []\n",
        "\n",
        "    # Quality insights\n",
        "    quality_ratio = df['quality_score'].mean()\n",
        "    if quality_ratio < 0.3:\n",
        "        insights.append(f\"Low quality comment ratio ({quality_ratio:.1%}). Consider content strategy review.\")\n",
        "    elif quality_ratio > 0.6:\n",
        "        insights.append(f\"High quality comment ratio ({quality_ratio:.1%}). Great audience engagement!\")\n",
        "\n",
        "    # Spam insights\n",
        "    spam_rate = df['isSpam'].mean()\n",
        "    if spam_rate > 0.2:\n",
        "        insights.append(f\"High spam rate ({spam_rate:.1%}). Implement stricter comment moderation.\")\n",
        "\n",
        "    # Sentiment insights\n",
        "    positive_ratio = (df['sentiment'] == 'positive').mean()\n",
        "    negative_ratio = (df['sentiment'] == 'negative').mean()\n",
        "\n",
        "    if positive_ratio > 0.5:\n",
        "        insights.append(f\"Positive sentiment dominates ({positive_ratio:.1%}). Audience responds well to content.\")\n",
        "    elif negative_ratio > 0.3:\n",
        "        insights.append(f\"High negative sentiment ({negative_ratio:.1%}). Review content strategy.\")\n",
        "\n",
        "    # Category insights\n",
        "    top_category = df['category'].value_counts().index[0]\n",
        "    top_category_pct = df['category'].value_counts(normalize=True).iloc[0]\n",
        "    insights.append(f\"ðŸ“Š '{top_category}' is the dominant category ({top_category_pct:.1%} of comments).\")\n",
        "\n",
        "    # Relevance insights\n",
        "    avg_relevance = df['relevance_score'].mean()\n",
        "    if avg_relevance < 0.1:\n",
        "        insights.append(f\"Low content relevance ({avg_relevance:.3f}). Comments may be off-topic.\")\n",
        "    elif avg_relevance > 0.3:\n",
        "        insights.append(f\"High content relevance ({avg_relevance:.3f}). Comments align well with video content.\")\n",
        "\n",
        "    return insights\n",
        "\n",
        "insights = generate_insights(comments)\n",
        "\n",
        "print(\"=== KEY INSIGHTS AND RECOMMENDATIONS ===\")\n",
        "for insight in insights:\n",
        "    print(insight)\n",
        "\n",
        "# Category-specific analysis\n",
        "print(\"\\n=== CATEGORY-SPECIFIC QUALITY ANALYSIS ===\")\n",
        "category_quality = comments.groupby('category').agg({\n",
        "    'quality_score': ['mean', 'count'],\n",
        "    'sentiment': lambda x: (x == 'positive').mean(),\n",
        "    'relevance_score': 'mean'\n",
        "}).round(3)\n",
        "\n",
        "category_quality.columns = ['Quality_Ratio', 'Comment_Count', 'Positive_Sentiment_Ratio', 'Avg_Relevance']\n",
        "print(category_quality)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omykzfHvl6-t"
      },
      "source": [
        "## 14. Export Results and Summary"
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-09-11T11:51:16.413501Z",
          "start_time": "2025-09-11T11:51:16.348064Z"
        },
        "id": "AmdN0y4Gl6-u"
      },
      "cell_type": "code",
      "source": [
        "# Create final summary dataframe\n",
        "summary_df = comments[[\n",
        "    'videoId', 'commentId', 'textOriginal', 'textCleaned',\n",
        "    'sentiment', 'sentiment_score', 'category',\n",
        "    'quality_score', 'isSpam', 'relevance_score'\n",
        "]].copy()\n",
        "\n",
        "# Add quality labels\n",
        "summary_df['quality_label'] = summary_df['quality_score'].map({0: 'Low Quality', 1: 'High Quality'})\n",
        "summary_df['spam_label'] = summary_df['isSpam'].map({0: 'Legitimate', 1: 'Spam'})\n",
        "\n",
        "print(\"=== FINAL SUMMARY ===\")\n",
        "print(f\"Dataset processed: {len(summary_df):,} comments\")\n",
        "print(f\"Analysis completed successfully!\")\n",
        "\n",
        "# Display sample of processed data\n",
        "print(\"\\n=== SAMPLE PROCESSED DATA ===\")\n",
        "display_cols = ['textOriginal', 'sentiment', 'category', 'quality_label', 'spam_label', 'relevance_score']\n",
        "sample_data = summary_df[display_cols].head(10)\n",
        "print(sample_data.to_string(max_colwidth=50))\n",
        "\n",
        "# Save results (uncomment to save)\n",
        "# summary_df.to_csv('comment_analysis_results.csv', index=False)\n",
        "# video_analytics.to_csv('video_analytics_summary.csv')\n",
        "# print(\"\\nResults saved to CSV files!\")\n",
        "\n",
        "print(\"\\nCommentSense AI Analysis Complete!\")\n",
        "print(\"\\nThe system has successfully analyzed comment quality, sentiment, categories, spam detection, and relevance at scale.\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_R0pgZRcl6-u"
      },
      "source": [
        "## 15. Model Performance Summary\n",
        "\n",
        "### Features Implemented:\n",
        "1. **Quality Comment Ratio Analysis** - Identifies high vs low quality comments based on multiple factors\n",
        "2. **Sentiment Breakdown** - Positive, negative, neutral sentiment analysis per video\n",
        "3. **Comment Categorization** - Skincare, makeup, fragrance, and other categories\n",
        "4. **Spam Detection** - Advanced spam detection using multiple indicators\n",
        "5. **Relevance Analysis** - Measures comment relevance to video content using cosine similarity\n",
        "6. **Interactive Dashboard** - Visual analytics for easy interpretation\n",
        "7. **Per-Video Analytics** - Detailed breakdown for each video\n",
        "8. **KPI Tracking** - Key performance indicators for content effectiveness\n",
        "\n",
        "### Key Metrics:\n",
        "- **Share of Engagement (SoE)** analysis through comment quality metrics\n",
        "- **Scalable processing** with batch analysis for large datasets\n",
        "- **Real-time insights** with actionable recommendations\n",
        "- **Category-specific analysis** for targeted content strategy\n",
        "\n",
        "This prototype demonstrates a comprehensive AI-powered solution for analyzing comment quality and relevance at scale, enabling data-driven decisions for content strategy optimization."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}