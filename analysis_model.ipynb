{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Video Analysis Model\n",
    "Put project desc here later\n",
    "\n",
    "By: **Noog Troupers**\n",
    "\n",
    "Members: enter later\n"
   ],
   "id": "c9c60a55cd0abf8c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load datasets",
   "id": "74f215646d684bfe"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T15:02:38.894339Z",
     "start_time": "2025-09-02T15:02:38.886039Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, pipeline, AutoTokenizer, AutoConfig\n",
    "from torch import nn\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer"
   ],
   "id": "e05f0f8dd8d668df",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T15:02:38.966318Z",
     "start_time": "2025-09-02T15:02:38.958318Z"
    }
   },
   "cell_type": "code",
   "source": [
    "comment_links = [\n",
    "    \"https://storage.googleapis.com/dataset_hosting/comments1.csv\",\n",
    "    \"https://storage.googleapis.com/dataset_hosting/comments2.csv\",\n",
    "    \"https://storage.googleapis.com/dataset_hosting/comments3.csv\",\n",
    "    \"https://storage.googleapis.com/dataset_hosting/comments4.csv\",\n",
    "    \"https://storage.googleapis.com/dataset_hosting/comments5.csv\",\n",
    "]\n",
    "\n",
    "video_link = \"https://storage.googleapis.com/dataset_hosting/videos.csv\"\n",
    "\n",
    "class Dataset:\n",
    "    @staticmethod\n",
    "    def getAllComments():\n",
    "        list_of_dfs = []\n",
    "        for csv_file in comment_links:\n",
    "            df = pd.read_csv(csv_file)\n",
    "            list_of_dfs.append(df)\n",
    "\n",
    "        return pd.concat(list_of_dfs, ignore_index=True)\n",
    "\n",
    "    @staticmethod\n",
    "    def getComments(dataset_id = 1):\n",
    "        if dataset_id not in range(1, len(comment_links)):\n",
    "            raise ValueError(\"dataset_id must be between 1 and 5\")\n",
    "\n",
    "        return pd.read_csv(comment_links[dataset_id - 1])\n",
    "\n",
    "    @staticmethod\n",
    "    def getVideos():\n",
    "        return pd.read_csv(video_link)\n",
    "\n",
    "dataset = Dataset()"
   ],
   "id": "2043610ba7fd191a",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-09-02T15:02:38.984345Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load video dataset\n",
    "videos = dataset.getVideos()\n",
    "\n",
    "# Load all comments (4.7 million comments use for actual training)\n",
    "# comments = dataset.getAllComments()\n",
    "\n",
    "# Load one comments dataset (~= 1 million comments each use for testing)\n",
    "comments = dataset.getComments(dataset_id = 1)\n",
    "comments = comments.sample(frac = 0.1, random_state = 42) # Use 10% of the data for testing"
   ],
   "id": "93667779917b1542",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T13:31:29.406542Z",
     "start_time": "2025-09-02T13:31:29.040038Z"
    }
   },
   "cell_type": "code",
   "source": [
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "class TextPreprocessor:\n",
    "    @staticmethod\n",
    "    def remove_stopwords(tokens):\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        return [word for word in tokens if word.lower() not in stop_words]\n",
    "\n",
    "    @staticmethod\n",
    "    def stem_tokens(tokens):\n",
    "        stemmer = PorterStemmer()\n",
    "        return [stemmer.stem(word) for word in tokens]\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_punctuation(tokens):\n",
    "        return [word for word in tokens if word not in string.punctuation]\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_special_characters(text):\n",
    "        return re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "\n",
    "    @staticmethod\n",
    "    def is_spam(text):\n",
    "        text = str(text).lower()\n",
    "        emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "\n",
    "        # If text is shorter than 5 characters without emojis consider it spam\n",
    "        text = emoji_pattern.sub(r'', text)\n",
    "        if len(text) < 5:\n",
    "            return int(True)\n",
    "\n",
    "        spam_keywords = ['buy now', 'click here', 'subscribe', 'free', 'visit', 'winner', 'win', 'cash', 'prize']\n",
    "\n",
    "        return int(any(keyword in text for keyword in spam_keywords))\n",
    "\n",
    "    def clean_text(self, text):\n",
    "        text = text.strip(' ')\n",
    "        text = text.lower()\n",
    "\n",
    "        tokens = word_tokenize(text)\n",
    "        filtered_tokens = self.remove_stopwords(tokens)\n",
    "        stemmed_tokens = self.stem_tokens(filtered_tokens)\n",
    "        punctuation_free_tokens = self.remove_punctuation(stemmed_tokens)\n",
    "        cleaned_text = ' '.join(punctuation_free_tokens)\n",
    "        cleaned_text = self.remove_special_characters(cleaned_text)\n",
    "\n",
    "\n",
    "        return cleaned_text"
   ],
   "id": "d54d5af50502ffaf",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Data Preprocessing",
   "id": "539e2e47f2e293a7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T13:31:44.398717Z",
     "start_time": "2025-09-02T13:31:44.318898Z"
    }
   },
   "cell_type": "code",
   "source": "comments.count()",
   "id": "7a70adfc6ba99edd",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "kind               100000\n",
       "commentId          100000\n",
       "channelId          100000\n",
       "videoId            100000\n",
       "authorId           100000\n",
       "textOriginal        99997\n",
       "parentCommentId     11033\n",
       "likeCount          100000\n",
       "publishedAt        100000\n",
       "updatedAt          100000\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T13:33:14.632769Z",
     "start_time": "2025-09-02T13:32:08.964967Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Remove duplicates\n",
    "comments = comments.drop_duplicates(subset = [\"commentId\"])\n",
    "videos = videos.drop_duplicates(subset = [\"videoId\"])\n",
    "\n",
    "# Note down spam comments\n",
    "comments[\"isSpam\"] = comments[\"textOriginal\"].apply(TextPreprocessor().is_spam)\n",
    "\n",
    "# Remove spam comments\n",
    "# comments = comments[comments[\"isSpam\"] == 0]\n",
    "# comments = comments.drop(columns = [\"isSpam\"])\n",
    "\n",
    "# Drop rows with missing comment text\n",
    "comments.dropna(inplace = True, subset = [\"textOriginal\"])\n",
    "\n",
    "# Clean comment text (remove stopwords, punctuation, special characters, and lowercase)\n",
    "comments[\"textCleaned\"] = comments[\"textOriginal\"].apply(TextPreprocessor().clean_text)"
   ],
   "id": "bab6a3512d0d13b8",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T13:33:19.493012Z",
     "start_time": "2025-09-02T13:33:19.477038Z"
    }
   },
   "cell_type": "code",
   "source": "comments",
   "id": "6a135b74a66766a8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                   kind  commentId  channelId  videoId  authorId  \\\n",
       "987231  youtube#comment     169483      45150    44543    773251   \n",
       "79954   youtube#comment    2141511      14429    69445   1911102   \n",
       "567130  youtube#comment    2544738      31438    32409   1832205   \n",
       "500891  youtube#comment    2873452      48537     6358    647305   \n",
       "55399   youtube#comment    4383408      14492    18248    525132   \n",
       "...                 ...        ...        ...      ...       ...   \n",
       "395942  youtube#comment    4183848      49533    52820   2736082   \n",
       "417771  youtube#comment    1314456      48953    29310    296154   \n",
       "937140  youtube#comment    4260332      23924    11515   3264222   \n",
       "794022  youtube#comment    3468699      41338    78245   1295913   \n",
       "573083  youtube#comment    2857569      28034    30055   1478066   \n",
       "\n",
       "                                             textOriginal  parentCommentId  \\\n",
       "987231              Thank you very much 🥰 Please share 🙏💞        4367972.0   \n",
       "79954   She looks pretty on both sides. Only big diffe...              NaN   \n",
       "567130  I hate straight hair & love it. Glad you like it❤              NaN   \n",
       "500891  The texture makes you look more beautiful and ...              NaN   \n",
       "55399                                            Handsome              NaN   \n",
       "...                                                   ...              ...   \n",
       "395942  Eu confiei e no meu dia menos estranha eu pare...              NaN   \n",
       "417771                               I'love indonesia❤❤❤❤              NaN   \n",
       "937140  Is it the eyebrow? Hair colour or her eyes.. D...              NaN   \n",
       "794022                  I LOVE my lanage it is so amazing              NaN   \n",
       "573083     You actually look like so-min from running man              NaN   \n",
       "\n",
       "        likeCount                publishedAt                  updatedAt  \\\n",
       "987231          0  2022-12-24 21:25:47+00:00  2022-12-24 21:25:47+00:00   \n",
       "79954           0  2024-08-01 11:49:33+00:00  2024-08-01 11:49:33+00:00   \n",
       "567130          0  2023-09-14 17:01:03+00:00  2023-09-14 17:01:03+00:00   \n",
       "500891        152  2024-10-01 17:27:48+00:00  2024-10-01 17:27:48+00:00   \n",
       "55399           0  2023-06-07 03:29:55+00:00  2023-06-07 03:29:55+00:00   \n",
       "...           ...                        ...                        ...   \n",
       "395942          0  2024-08-26 11:11:24+00:00  2024-08-26 11:11:24+00:00   \n",
       "417771          0  2023-11-03 03:50:54+00:00  2023-11-03 03:50:54+00:00   \n",
       "937140          0  2023-10-05 21:14:24+00:00  2023-10-05 21:14:24+00:00   \n",
       "794022          0  2024-12-27 14:35:35+00:00  2024-12-27 14:35:35+00:00   \n",
       "573083          0  2022-01-07 05:33:35+00:00  2022-01-07 05:33:35+00:00   \n",
       "\n",
       "        isSpam                                        textCleaned  \n",
       "987231       0                           thank much  pleas share   \n",
       "79954        0                look pretti side big differ eye see  \n",
       "567130       0               hate straight hair love glad like it  \n",
       "500891       0                       textur make look beauti aliv  \n",
       "55399        0                                            handsom  \n",
       "...        ...                                                ...  \n",
       "395942       0  eu confiei e meu dia meno estranha eu pareo mu...  \n",
       "417771       0                                     ilov indonesia  \n",
       "937140       0  eyebrow hair colour eye  nt want shame suddenl...  \n",
       "794022       0                                    love lanag amaz  \n",
       "573083       0                     actual look like somin run man  \n",
       "\n",
       "[99997 rows x 12 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>kind</th>\n",
       "      <th>commentId</th>\n",
       "      <th>channelId</th>\n",
       "      <th>videoId</th>\n",
       "      <th>authorId</th>\n",
       "      <th>textOriginal</th>\n",
       "      <th>parentCommentId</th>\n",
       "      <th>likeCount</th>\n",
       "      <th>publishedAt</th>\n",
       "      <th>updatedAt</th>\n",
       "      <th>isSpam</th>\n",
       "      <th>textCleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>987231</th>\n",
       "      <td>youtube#comment</td>\n",
       "      <td>169483</td>\n",
       "      <td>45150</td>\n",
       "      <td>44543</td>\n",
       "      <td>773251</td>\n",
       "      <td>Thank you very much 🥰 Please share 🙏💞</td>\n",
       "      <td>4367972.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-12-24 21:25:47+00:00</td>\n",
       "      <td>2022-12-24 21:25:47+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>thank much  pleas share</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79954</th>\n",
       "      <td>youtube#comment</td>\n",
       "      <td>2141511</td>\n",
       "      <td>14429</td>\n",
       "      <td>69445</td>\n",
       "      <td>1911102</td>\n",
       "      <td>She looks pretty on both sides. Only big diffe...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-08-01 11:49:33+00:00</td>\n",
       "      <td>2024-08-01 11:49:33+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>look pretti side big differ eye see</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567130</th>\n",
       "      <td>youtube#comment</td>\n",
       "      <td>2544738</td>\n",
       "      <td>31438</td>\n",
       "      <td>32409</td>\n",
       "      <td>1832205</td>\n",
       "      <td>I hate straight hair &amp; love it. Glad you like it❤</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-09-14 17:01:03+00:00</td>\n",
       "      <td>2023-09-14 17:01:03+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>hate straight hair love glad like it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500891</th>\n",
       "      <td>youtube#comment</td>\n",
       "      <td>2873452</td>\n",
       "      <td>48537</td>\n",
       "      <td>6358</td>\n",
       "      <td>647305</td>\n",
       "      <td>The texture makes you look more beautiful and ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>152</td>\n",
       "      <td>2024-10-01 17:27:48+00:00</td>\n",
       "      <td>2024-10-01 17:27:48+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>textur make look beauti aliv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55399</th>\n",
       "      <td>youtube#comment</td>\n",
       "      <td>4383408</td>\n",
       "      <td>14492</td>\n",
       "      <td>18248</td>\n",
       "      <td>525132</td>\n",
       "      <td>Handsome</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-06-07 03:29:55+00:00</td>\n",
       "      <td>2023-06-07 03:29:55+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>handsom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395942</th>\n",
       "      <td>youtube#comment</td>\n",
       "      <td>4183848</td>\n",
       "      <td>49533</td>\n",
       "      <td>52820</td>\n",
       "      <td>2736082</td>\n",
       "      <td>Eu confiei e no meu dia menos estranha eu pare...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-08-26 11:11:24+00:00</td>\n",
       "      <td>2024-08-26 11:11:24+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>eu confiei e meu dia meno estranha eu pareo mu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417771</th>\n",
       "      <td>youtube#comment</td>\n",
       "      <td>1314456</td>\n",
       "      <td>48953</td>\n",
       "      <td>29310</td>\n",
       "      <td>296154</td>\n",
       "      <td>I'love indonesia❤❤❤❤</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-11-03 03:50:54+00:00</td>\n",
       "      <td>2023-11-03 03:50:54+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>ilov indonesia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>937140</th>\n",
       "      <td>youtube#comment</td>\n",
       "      <td>4260332</td>\n",
       "      <td>23924</td>\n",
       "      <td>11515</td>\n",
       "      <td>3264222</td>\n",
       "      <td>Is it the eyebrow? Hair colour or her eyes.. D...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-10-05 21:14:24+00:00</td>\n",
       "      <td>2023-10-05 21:14:24+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>eyebrow hair colour eye  nt want shame suddenl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>794022</th>\n",
       "      <td>youtube#comment</td>\n",
       "      <td>3468699</td>\n",
       "      <td>41338</td>\n",
       "      <td>78245</td>\n",
       "      <td>1295913</td>\n",
       "      <td>I LOVE my lanage it is so amazing</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-12-27 14:35:35+00:00</td>\n",
       "      <td>2024-12-27 14:35:35+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>love lanag amaz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>573083</th>\n",
       "      <td>youtube#comment</td>\n",
       "      <td>2857569</td>\n",
       "      <td>28034</td>\n",
       "      <td>30055</td>\n",
       "      <td>1478066</td>\n",
       "      <td>You actually look like so-min from running man</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-01-07 05:33:35+00:00</td>\n",
       "      <td>2022-01-07 05:33:35+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>actual look like somin run man</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>99997 rows × 12 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T14:05:12.364182Z",
     "start_time": "2025-09-02T14:05:09.254050Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# determine device for the pipeline (use GPU if available)\n",
    "device_index = 0 if torch.cuda.is_available() else -1\n",
    "# model = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "analyzer = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=model,\n",
    "    truncation=True,\n",
    "    device=device_index,\n",
    ")"
   ],
   "id": "ad4a686ac6655eb3",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T14:14:54.587371Z",
     "start_time": "2025-09-02T14:07:45.724908Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Faster batched inference (and avoid re-processing duplicate texts)\n",
    "# 1) map NaNs to empty strings and ensure str dtype\n",
    "texts_series = comments[\"textCleaned\"].fillna(\"\").astype(str)\n",
    "\n",
    "# 2) run inference only on unique texts to reduce duplicate work\n",
    "unique_texts = list(pd.Series(texts_series.unique()))\n",
    "\n",
    "# 3) infer labels in batches and build a mapping\n",
    "batch_size = 64  # tune this based on available memory/GPU\n",
    "label_map = {}\n",
    "for i in range(0, len(unique_texts), batch_size):\n",
    "    batch = unique_texts[i:i + batch_size]\n",
    "    out = analyzer(batch, truncation=True, max_length = 512, batch_size=len(batch))\n",
    "    for text, res in zip(batch, out):\n",
    "        label_map[text] = res[\"label\"]\n",
    "\n",
    "# 4) map back to the dataframe\n",
    "comments[\"sentiment\"] = texts_series.map(label_map)"
   ],
   "id": "a69500c1e35e891d",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T14:25:15.362333Z",
     "start_time": "2025-09-02T14:25:15.347990Z"
    }
   },
   "cell_type": "code",
   "source": "comments[[\"textCleaned\", \"sentiment\"]]",
   "id": "81fb358b0785dd28",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                              textCleaned sentiment\n",
       "987231                           thank much  pleas share   positive\n",
       "79954                 look pretti side big differ eye see   neutral\n",
       "567130               hate straight hair love glad like it  positive\n",
       "500891                       textur make look beauti aliv  positive\n",
       "55399                                             handsom  positive\n",
       "...                                                   ...       ...\n",
       "395942  eu confiei e meu dia meno estranha eu pareo mu...   neutral\n",
       "417771                                     ilov indonesia   neutral\n",
       "937140  eyebrow hair colour eye  nt want shame suddenl...   neutral\n",
       "794022                                    love lanag amaz  positive\n",
       "573083                     actual look like somin run man   neutral\n",
       "\n",
       "[99997 rows x 2 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textCleaned</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>987231</th>\n",
       "      <td>thank much  pleas share</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79954</th>\n",
       "      <td>look pretti side big differ eye see</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567130</th>\n",
       "      <td>hate straight hair love glad like it</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500891</th>\n",
       "      <td>textur make look beauti aliv</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55399</th>\n",
       "      <td>handsom</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395942</th>\n",
       "      <td>eu confiei e meu dia meno estranha eu pareo mu...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417771</th>\n",
       "      <td>ilov indonesia</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>937140</th>\n",
       "      <td>eyebrow hair colour eye  nt want shame suddenl...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>794022</th>\n",
       "      <td>love lanag amaz</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>573083</th>\n",
       "      <td>actual look like somin run man</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>99997 rows × 2 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
